{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45fd35b-85be-463e-bcd7-7f74c7c064d8",
   "metadata": {},
   "source": [
    "# Serving Large Language Models (LLMs) at Scale on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971cc67-4580-4a79-a6d2-8daaa8291951",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Language Model (LLM) serving at scale refers to the process of delivering an LLM as a service to a large number of users efficiently. This involves setting up a robust and efficient infrastructure to handle a large number of requests quickly and at a low latency.\n",
    "\n",
    "LLM inference at scale can be achieved through various techniques such as model parallelism, data parallelism, efficient data handling, optimized inference, asynchronous inference, auto-scaling, load balancing, optimized hardware, caching, and using model serving frameworks. The goal is to minimize the latency of individual requests, reduce the computational complexity of the model, and improve the overall system throughput to meet the demands of a larger number of users. \n",
    "\n",
    "By serving an LLM at scale, you can make it accessible to a wider audience and enable them to use the model to generate text, translate languages, answer questions, and perform other natural language processing tasks quickly and efficiently.\n",
    "\n",
    "In this article, I will use Amazon SageMaker to show how we can control resources to serve our LLM models at scale. It may include the number of GPUs, memory, or the replicas assigned to serve dynamic amounts of requests to our LLM models. Moreover, I also show how to attach an auto-scaling policy to our serving endpoint which will scale our endpoint automatically when workload varies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be1ebb-d9a3-42f5-adf2-689994cf4eb3",
   "metadata": {},
   "source": [
    "## AWS Deep Learning Containers\n",
    "\n",
    "\n",
    "Large Language Models (LLMs) have become a forefront of innovation in artificial intelligence, capturing the attention of academic establishments, tech companies and enthusiasts with their sophisticated capabilities. Models built on architectures like GPT and Llama have rapidly gained traction for a wide range of uses such as language comprehension, conversational interfaces, and automated content creation. This surge in demand has led many companies to explore and integrate LLM-driven features into their products.\n",
    "\n",
    "However, deploying LLMs on a large scale involves complex engineering challenges. To ensure a seamless user experience, hosting services for LLMs need to maintain quick response times while supporting numerous users simultaneously. Due to the substantial resource demands of these models, standard inference frameworks often fall short in delivering the necessary optimizations for optimal resource use and performance.\n",
    "\n",
    "Key optimizations that can enhance LLM hosting include:\n",
    "\n",
    "* Tensor parallelism, which spreads computation across multiple processing units.\n",
    "* Model quantization, which reduces the modelâ€™s memory usage.\n",
    "* Dynamic batching of requests to increase processing throughput and more.\n",
    "\n",
    "\n",
    "Recently, AWS has released a new Hugging Face Deep Learning Container (DLC) for inference with Large Language Models (LLMs). This new Hugging Face LLM DLC is powered by Text Generation Inference (TGI), an open source, purpose-built solution for deploying and serving Large Language Models. TGI enables high-performance text generation using Tensor Parallelism and dynamic batching for the most popular open-source LLMs. The Hugging Face LLM DLC incorporates all the aforementioned optimizations as standard features, simplifying the large-scale deployment of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ad4f0-4f1e-405f-b7e9-345d33463d79",
   "metadata": {},
   "source": [
    "# Serving Llama 3 at Scale in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab88b9-cc3c-4ed1-88e9-bbfe4115c0f5",
   "metadata": {},
   "source": [
    "Let's start off by installing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139c7a8-0c79-480a-9d29-e11fd4e418e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33078531-ce7c-4476-b7b0-9bf8216b9e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::609362070692:role/service-role/AmazonSageMaker-ExecutionRole-20231122T115899\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  \n",
    "sess = sagemaker.session.Session() \n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68d71a-300a-42c8-be9f-5bd4218acbaa",
   "metadata": {},
   "source": [
    "Next, we need to retrieve the container uri and provide it to our HuggingFaceModel model class with a image_uri pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the <code>get_huggingface_llm_image_uri</code> method provided by the sagemaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e14815-19d4-4e8e-9ee7-fb52127312dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=\"Your Hugging Face access token\")\n",
    "\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    ")\n",
    "\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31753a-2dbe-4f58-a0df-244bf624dca3",
   "metadata": {},
   "source": [
    "To deploy Llama 3 70B to Amazon SageMaker we create a <code>HuggingFaceModel</code> model class and define our endpoint configuration including the hf_model_id, instance_type etc. We will use a g5.45xlarge instance type, which has 8 NVIDIA A10G GPUs and 192GB of GPU memory. You need atleast > 100GB of GPU memory to run Mixtral 8x7B in float16 with decent input length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce25cf3-7903-41f0-852e-1c9291ef5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.48xlarge\"\n",
    "number_of_gpu = 8\n",
    "health_check_timeout = 300\n",
    " \n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"NousResearch/Llama-2-7b-chat-hf\", #  \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "  'SM_NUM_GPUS': \"1\", # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"4096\",  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': \"8192\", # Limits the number of tokens that can be processed in parallel during the generation. \n",
    "\n",
    "}\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf18a3-913a-4baf-b976-277e6695fcae",
   "metadata": {},
   "source": [
    "# Multi-model deployment on SageMaker\n",
    "Multi-model endpoints offer a scalable and cost-effective way to deploy models. Multi-model endpoints use a single set of resources and a shared serving container to host all the models together. This shared setup allows the models to efficiently use the same hardware, which helps lower hosting costs. By improving endpoint's resource usage, multi-model endpoints are more cost-effective than deploying each model on its own separate endpoint. Figure 1 compares single-model endpoint and multi-model endpoint. You can use Amazon SageMaker to deploy one or more models to an endpoint. If you deploy multiple models to the same endpoint, they will share the resources available there, including ML compute instances, CPUs, and accelerators. This means that all the models will use the same hardware resources for their computations. The most flexible way to deploy multiple models to an endpoint is to define each model as an <i>inference component</i>. \n",
    "\n",
    "## Inference components\n",
    "An inference component is a SageMaker hosting object that you can use to deploy a model to an endpoint. In the inference component settings, you specify the model, the endpoint, and how the model utilizes the resources that the endpoint hosts. \n",
    "\n",
    "With <code>ResourceRequirements</code> you can assign endpoint resources to a model. These resources include CPU cores, accelerators, and memory. Let's see them in code:\n",
    "\n",
    "\n",
    "<center><figure><img src=\"imgs/SME-MME.png\" alt=\"drawing\" width=\"800\"/><figcaption>Fig. 1: single-model endpoint vs. multi-model endpoint</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b588ee2-1ffa-41ec-badc-a1cdfd3c14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "llama3_resource_config = ResourceRequirements(\n",
    "    requests = {\n",
    "        \"copies\": 8, #1\n",
    "        \"num_accelerators\": 1, #2\n",
    "        \"num_cpus\": 18,  #3 \n",
    "        \"memory\": 72 * 1024,  #4\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe626e1-6565-463b-83f2-7da96b13ae0b",
   "metadata": {},
   "source": [
    "In the above configuration, #1 is the number of copies of the model, #2 define the number of GPUs to dedicate to the model, #3 is the number of CPUs to be assigned to the model, and #4 is the amount of memory dedicated to the model. But, what are the right values in this configuration? Since we defined 8 replicas, we need to divide our resource by 8. That is 1 GPU per replica. However, SageMaker sets aside some CPU and memory resources for task management. More precisely, It reserves 2 units for this purpose. For example, ml.g5.48xlarge instance has 192 CPUs. 192/8 = 24. Therefore, we will have: $(192- (2*24))/8 = 18$ CPUs and $(768- (2*768/8))/8 = 72 KB$ memory for each replica. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9500a-8190-4ce6-b76d-9e92d7c153e8",
   "metadata": {},
   "source": [
    "Finally, we deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee1277-b9b5-478c-bf17-fa293741491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!-----------------------------------------------------!CPU times: user 373 ms, sys: 46.6 ms, total: 419 ms\n",
      "Wall time: 21min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from sagemaker.enums import EndpointType\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type, \n",
    "    resources=llama3_resource_config, \n",
    "    container_startup_health_check_timeout=health_check_timeout, #1\n",
    "    endpoint_name=f\"llama3-chat-{str(uuid.uuid4())}\",\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED, #2\n",
    "    tags=[{\"Key\": \"aKey\", \"Value\": \"aValue\"}],\n",
    "    model_name=\"llama3-chat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4af2e-6457-44a3-ad72-dfc3fbe22c1f",
   "metadata": {},
   "source": [
    "In above: #1 Specifies the health checkup timeout in seconds which is the timeout the container has to respond to health checks. If CloudWatch logs indicate a health check timeout, you should increase this quota. #2 inorder to attach the resource requirement, the endpoint type should be an inference component as we discussed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2769-8311-4179-9c4d-806dc452a5fe",
   "metadata": {},
   "source": [
    "# AutoScaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5fef6-efb1-4587-af81-72eae2fa987c",
   "metadata": {},
   "source": [
    "Amazon SageMaker offers automatic scaling (auto scaling) for hosted models, which dynamically adjusts the number of instances based on workload changes. When the workload increases, auto scaling activates additional instances. Conversely, when the workload decreases, it removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.\n",
    "\n",
    "First, we need to define a scaling policy that adds and removes the number of instances for our production endpoint in response to workload changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af91ed5-a9ce-4145-b489-37dfecea0cc3",
   "metadata": {},
   "source": [
    "# Autoscaling Policies\n",
    "\n",
    "There are three main types of autoscaling policies for SageMaker Endpoints: target tracking, simple, and step scaling:\n",
    "\n",
    "* Target Tracking:\n",
    "With the target tracking scaling policy, you choose an Amazon CloudWatch metric and target value, such as SageMaker VariantInvocationsPerInstance = 100, and SageMaker can keep VariantInvocationsPerInstance at, or close to 100. This approach is very common due to its ease of configuration.\n",
    "\n",
    "* Simple Scaling:\n",
    "The simple scaling policy triggers a scaling event based on a specified metric at a defined threshold with a fixed amount of scaling. For instance, \"when SageMaker VariantInvocationsPerInstance > 1000, add 10 instances.\" This strategy requires more configuration but offers greater control compared to target tracking.\n",
    "\n",
    "* Step Scaling:\n",
    "You can use step scaling when you require an advanced configuration, such as specifying how many instances to deploy under what conditions. For example, \"when SageMaker VariantInvocationsPerInstance > 1000, add 10 instances; when SageMaker VariantInvocationsPerInstance > 2000, add 50 instances.\" This approach demands the most configuration but provides the highest level of control, especially for handling spiky traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd150025-10bf-48a0-8991-2fdc24d6b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale = boto3.Session().client(service_name=\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4505a86-3b4f-4043-abbc-8ab9c60d3c59",
   "metadata": {},
   "source": [
    "First, we need to register the resource as a scalable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e3925aa-9741-4a85-9431-71fc3f2d52bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ScalableTargetARN': 'arn:aws:application-autoscaling:us-east-1:609362070692:scalable-target/056mc0015e0cc4ea489497ea06e58faa340c',\n",
       " 'ResponseMetadata': {'RequestId': 'caa5d34c-9b7f-4fee-886a-04588d39e217',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'caa5d34c-9b7f-4fee-886a-04588d39e217',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '131',\n",
       "   'date': 'Tue, 28 May 2024 23:12:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = llm.endpoint_name\n",
    "autoscale.register_scalable_target(    \n",
    "    ServiceNamespace=\"sagemaker\", #1\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\", #2\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\", #3\n",
    "    MinCapacity=1, #4\n",
    "    MaxCapacity=2, #5\n",
    "    RoleARN=role,\n",
    "    SuspendedState={\n",
    "        \"DynamicScalingInSuspended\": False,\n",
    "        \"DynamicScalingOutSuspended\": False,\n",
    "        \"ScheduledScalingSuspended\": False,\n",
    "        \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c409362-e3e3-4275-96ba-eae6046c80f5",
   "metadata": {},
   "source": [
    "In the above configuration, we have defined #1) The AWS service name, #2) The identifier of the resource that is associated with the scalable target. #3) The scalable property associated with the scalable target (e.g., the number of EC2 instances for a SageMaker model endpoint variant) #4, #5) The minimum/maximum value that you plan to scale in/ scale out to. Please refere to the [documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling/client/register_scalable_target.html) for other configurable parameters.\n",
    "\n",
    "After registering sagemaker as a scalable target, we can create or update the scaling policy for that target using <code>put_scaling_policy</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1080813-5d22-4e7b-a8e7-ef9c4d115a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PolicyARN': 'arn:aws:autoscaling:us-east-1:609362070692:scalingPolicy:c0015e0c-c4ea-4894-97ea-06e58faa340c:resource/sagemaker/endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic:policyName/autoscale-policy-llama3-8b',\n",
       " 'Alarms': [{'AlarmName': 'TargetTracking-endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic-AlarmHigh-e3070ea0-e84e-4491-b1e7-6bef30bdb5da',\n",
       "   'AlarmARN': 'arn:aws:cloudwatch:us-east-1:609362070692:alarm:TargetTracking-endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic-AlarmHigh-e3070ea0-e84e-4491-b1e7-6bef30bdb5da'},\n",
       "  {'AlarmName': 'TargetTracking-endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic-AlarmLow-6c117972-4125-40a9-a1ed-1ae17f75cb73',\n",
       "   'AlarmARN': 'arn:aws:cloudwatch:us-east-1:609362070692:alarm:TargetTracking-endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic-AlarmLow-6c117972-4125-40a9-a1ed-1ae17f75cb73'}],\n",
       " 'ResponseMetadata': {'RequestId': 'b54396f9-b9ef-46eb-8c7d-078fd216b182',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b54396f9-b9ef-46eb-8c7d-078fd216b182',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '962',\n",
       "   'date': 'Tue, 28 May 2024 23:12:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoscale.put_scaling_policy(\n",
    "    PolicyName=\"autoscale-policy-llama3-8b\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 400, # 400% of 800% total GPU utilization (8 GPUs)\n",
    "        \"CustomizedMetricSpecification\":\n",
    "        {\n",
    "            \"MetricName\": \"GPUUtilization\",\n",
    "            \"Namespace\": \"/aws/sagemaker/Endpoints\",\n",
    "            \"Dimensions\": [\n",
    "                {\"Name\": \"EndpointName\", \"Value\": endpoint_name },\n",
    "                {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"}\n",
    "            ],\n",
    "            \"Statistic\": \"Average\",\n",
    "            \"Unit\": \"Percent\"\n",
    "        },\n",
    "        \"ScaleOutCooldown\": 60,\n",
    "        \"ScaleInCooldown\": 300,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d5897-9eee-4985-9c3c-5c00e4beb885",
   "metadata": {},
   "source": [
    "Here, we need to define PolicyType which will be one of the auto-scaling policies described earlier. We define Target Value which acts as a metric trigger to scale out or scale in the defined scalable dimension (e.g., sagemaker:variant:DesiredInstanceCount). For example in the above configuration, when GPUUtilization exceeds 400 percent a new instance is added to our deployed endpoint. \n",
    "\n",
    "A cooldown period defines the time interval the scaling policy waits before initiating another scaling action. This mechanism helps prevent over-scaling.\n",
    "\n",
    "<code>ScaleOutCooldown</code>: Following a successful scale-out by the scaling policy, auto scaler begins calculating the cooldown period. The policy will not increase the desired capacity again unless a more significant scale-out event occurs or the cooldown period finishes.\n",
    "\n",
    "<code>ScaleInCooldown</code>: To ensure application availability, upcoming scale-in activities are suspended until the scale-in cooldown period has ended. Default value is 300 seconds for both of them.\n",
    "\n",
    "\n",
    "Figure 2 shows how auto-scaling works. \n",
    "\n",
    "<center><figure><img src=\"imgs/scaling.png\" alt=\"drawing\" width=\"800\"/><figcaption>Fig. 2: Auto-scaling Policy</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0df10-fca1-4e62-9a2b-31753d501a19",
   "metadata": {},
   "source": [
    "## Trigger autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8918d-f928-41ab-8f7e-4f074673b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"HF_MODEL_ID\"])\n",
    "\n",
    "# Conversational messages\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are an helpful Travel Assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Where is a good vacation destination in north america for summer?\"},\n",
    "]\n",
    "\n",
    "# generation parameters\n",
    "parameters = {    \n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "}\n",
    "\n",
    "for i in range(0, 100):\n",
    "    res = llm.predict(\n",
    "      {\n",
    "        \"inputs\": tokenizer.apply_chat_template(messages, tokenize=False),\n",
    "        \"parameters\": parameters\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1b115-1c38-47e1-963f-5064243d321b",
   "metadata": {},
   "source": [
    "In the above code snippet, <code>apply_chat_template</code> with convert the message to a chat template of the underlying LLM. Figure 3 shows the result of this code (endpoint triggering) on CloudWatch dashboard.\n",
    "\n",
    "\n",
    "\n",
    "<center><figure><img src=\"imgs/cloud-watch.png\" alt=\"drawing\" width=\"1000\"/><figcaption>Fig. 3: GPU utilization of the endpoint</figcaption></figure></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0838ace-b1da-4da0-bd49-8b3e66c27cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ScalingActivities': [{'ActivityId': '7bbe8709-4519-4e52-99f1-a0da6a87ad26',\n",
       "   'ServiceNamespace': 'sagemaker',\n",
       "   'ResourceId': 'endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic',\n",
       "   'ScalableDimension': 'sagemaker:variant:DesiredInstanceCount',\n",
       "   'Description': 'Setting desired instance count to 2.',\n",
       "   'Cause': 'monitor alarm TargetTracking-endpoint/llama3-chat-7933751b-cce5-40ed-b7a4-ab1a9c182a04/variant/AllTraffic-AlarmHigh-e3070ea0-e84e-4491-b1e7-6bef30bdb5da in state ALARM triggered policy autoscale-policy-llama3-8b',\n",
       "   'StartTime': datetime.datetime(2024, 5, 28, 23, 16, 11, 162000, tzinfo=tzlocal()),\n",
       "   'StatusCode': 'InProgress',\n",
       "   'StatusMessage': 'Successfully set desired instance count to 2. Waiting for change to be fulfilled by sagemaker.'}],\n",
       " 'ResponseMetadata': {'RequestId': 'a2d5fab8-819c-4d1a-966e-15852efec88e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a2d5fab8-819c-4d1a-966e-15852efec88e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '704',\n",
       "   'date': 'Tue, 28 May 2024 23:18:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoscale.describe_scaling_activities(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + endpoint_name + \"/variant/AllTraffic\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MaxResults=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c3028-5609-4991-88e9-c6c990a78d7c",
   "metadata": {},
   "source": [
    "Looking closely at the output, we can see that the number of instances are successfully increased to 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f78df3-e556-4288-8b4b-baf4487b6330",
   "metadata": {},
   "source": [
    "# Experiment cost\n",
    "\n",
    "As you can see it takes about 22 minutes to deploy the model to an endpoint. The loop to trigger auto-scaling takes around 4 minutes. Usage rate of a <code>ml.g5.48xlarge</code> instance is $\\$20.360/hr$ which will be around $\\$7.46$ to deploy the model and $\\$1.35$ for the auto-scaling triggering loop. Considering the whole experiment, it will cost you around $\\$10$ to successfully run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105096ef-a224-4138-af2e-041ff069ca4c",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "We need to delete the inference component, the model, and the endpoint to stop any unwilling charges. Otherwise, since this is an expensive instance, you will pay for your forgetfulness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4431600f-af68-4dc0-a217-3deae19c3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component = llm_model.sagemaker_session.list_inference_components(endpoint_name_equals=llm.endpoint_name).get(\"InferenceComponents\")[0].get(\"InferenceComponentName\")\n",
    "llm_model.sagemaker_session.delete_inference_component(inference_component_name=inference_component)\n",
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee808c-af2b-45b3-bbfa-71b71864d0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
